{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = './data/train'\n",
    "train_labels = './data/train.csv'\n",
    "val_data = './data/val'\n",
    "val_labels = './data/val.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities import GestureDataGenerator, plot_training_history, set_seed, get_callbacks, checks_and_balances\n",
    "checks_and_balances()\n",
    "set_seed(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_fraction = 1.0\n",
    "batch_size = 64\n",
    "image_size = (224, 224)\n",
    "\n",
    "# Initialize the generator\n",
    "train_generator = GestureDataGenerator(\n",
    "    data_path=train_data,\n",
    "    labels_csv=train_labels,\n",
    "    batch_size=batch_size,\n",
    "    image_size=image_size,\n",
    "    augmentations={\n",
    "        'rotation': 5,       # Rotate up to Â±5 degrees\n",
    "        'brightness': True,  # Random brightness adjustment\n",
    "        'contrast': True,    # Random contrast adjustment\n",
    "        'scaling': True,     # Random scaling (zoom)\n",
    "        'translation': True, # Random translation (shift)\n",
    "        'blur': True         # Apply Gaussian blur\n",
    "    },    \n",
    "    shuffle=True,\n",
    "    load_fraction=load_fraction,\n",
    "    debug=False,\n",
    "    use_mediapipe=False,\n",
    "    # workers=4,\n",
    "    # use_multiprocessing=True,\n",
    "    # max_queue_size=10\n",
    ")\n",
    "\n",
    "# # Get the first batch\n",
    "X, y = train_generator[0]\n",
    "\n",
    "# # Print outputs\n",
    "print(\"Input batch shape (X):\", X.shape)  # Expected shape: (batch_size, sequence_length, 224, 224, 3)\n",
    "print(\"Labels batch shape (y):\", y.shape)  # Expected shape: (batch_size, num_classes)\n",
    "print(\"First label in batch (one-hot):\", y[0])\n",
    "\n",
    "val_generator = GestureDataGenerator(\n",
    "    data_path=val_data,\n",
    "    labels_csv=val_labels,\n",
    "    batch_size=batch_size,\n",
    "    image_size=image_size,\n",
    "    augmentations=None,\n",
    "    shuffle=False,\n",
    "    load_fraction=1.0,\n",
    "    debug=False,\n",
    "    use_mediapipe=False,\n",
    "    # workers=4,\n",
    "    # use_multiprocessing=True,\n",
    "    # max_queue_size=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "sequence_length = train_generator.sequence_length  # Frames per video (from generator)\n",
    "image_size = train_generator.image_size            # Image size (height, width)\n",
    "num_classes = train_generator.num_classes          # Number of gesture classes\n",
    "input_shape = (sequence_length, image_size[0], image_size[1], 3)  # input shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated Pretrained CNN as feature extractor\n",
    "from tensorflow.keras.applications import MobileNetV2\n",
    "\n",
    "# Pretrained CNN as feature extractor\n",
    "cnn_base = MobileNetV2(weights=\"imagenet\", include_top=False, input_shape=(image_size[0], image_size[1], 3))\n",
    "cnn_base.trainable = False  # Allow fine-tuning of the top layers\n",
    "cnn_base.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import TimeDistributed, GRU, Dropout, Dense, MaxPooling2D, Flatten, Input\n",
    "\n",
    "# Updated model\n",
    "model = Sequential([\n",
    "    Input(shape=(30, 224, 224, 3), name=\"Input_Layer\"),       # Input layer for sequences of 30 frames\n",
    "    TimeDistributed(cnn_base, name=\"CNN_Layer\"),              # CNN base to process each frame independently\n",
    "    TimeDistributed(MaxPooling2D(), name=\"MaxPooling_Layer\"), # Max Pooling to reduce spatial dimensions\n",
    "    TimeDistributed(Flatten(), name=\"Flatten_Layer\"),         # Flatten spatial dimensions into feature vectors\n",
    "    GRU(32, return_sequences=False, name=\"GRU_Layer\"),        # GRU for temporal feature extraction\n",
    "    Dropout(0.5, name=\"Dropout_Layer\"),                       # Dropout for regularization\n",
    "    Dense(5, activation=\"softmax\", name=\"Output_Layer\")       # Dense layer for 5 gesture classes\n",
    "], name=\"CNN_GRU_Model\")\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam',\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_location = 'best-models/pretrained-MobileNetV2+GRU.keras'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback, reduce_lr_callback, early_stopping_callback = get_callbacks(filepath = model_save_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history_1 = model.fit(\n",
    "    x=train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=30,\n",
    "    callbacks=[checkpoint_callback, reduce_lr_callback, early_stopping_callback],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "plot_training_history(histories=[history_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model.load_weights(model_save_location)\n",
    "\n",
    "# # Train the model for more epochs\n",
    "# history_2 = model.fit(\n",
    "#     x=train_generator,\n",
    "#     validation_data=val_generator,\n",
    "#     epochs=50,                   # Train for more epochs\n",
    "#     initial_epoch=30,            # Start counting previous epochs\n",
    "#     callbacks=[checkpoint_callback, reduce_lr_callback, early_stopping_callback],\n",
    "#     verbose=1\n",
    "# )\n",
    "\n",
    "# plot_training_history(histories=[history_1, history_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(model_save_location)\n",
    "\n",
    "evaluation_results = model.evaluate(val_generator, verbose=1)\n",
    "\n",
    "for metric, value in zip(model.metrics_names, evaluation_results):\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Update you test data path here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = './data/val'\n",
    "test_labels = './data/val.csv'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Performance Summary Table\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| **Index** | **Model**                     | **Key Hyperparameters**                                                                 | **Results**                             | **Conclusion & Explanation**                                                                                                                                                                                                                                                                                                       |\n",
    "|-----------|--------------------------------|-----------------------------------------------------------------------------------------|------------------------------------------|-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| 1         | Conv2D + GRU                  | Conv2D (16, 32, 64 filters), GRU (64 units), Dense (5 units), Dropout (40%)             | Accuracy: Train 98.79%, Val 94.0%        | The model learns spatial and temporal features effectively. Conv2D layers capture spatial features, while GRU layers handle temporal dependencies. Regularization with dropout minimizes overfitting, enabling strong generalization. This model performs robustly due to the balance of spatial and temporal learning.                   |\n",
    "| 2         | Conv2D + LSTM                 | Conv2D (16, 32, 64 filters), LSTM (32 units), Dense (5 units), Dropout (50%)            | Accuracy: Train 92.01%, Val 91.0%        | Conv2D layers extract spatial patterns effectively, while the LSTM layer captures temporal dependencies. However, its slightly lower accuracy compared to Conv2D+GRU indicates GRUs might better capture the temporal nuances for gesture recognition tasks. Dropout helps reduce overfitting.                                           |\n",
    "| 3         | Conv3D Without Pretraining     | 3 Conv3D layers (filters: 32, 64, 128), GlobalAvgPooling, Dense (128 units, dropout 50%) | Accuracy: Train 93.82%, Val 89.0%        | The Conv3D model effectively learns spatial and temporal features. Its consistent performance shows that raw spatial-temporal features are well captured without needing a pretrained base. The dropout layer effectively regularizes training. Further optimization could focus on data augmentation.                              |\n",
    "| 4         | GRU With MediaPipe Keypoints   | GRU (64 units), Flatten, Dense (5 units), Dropout (50%)                                  | Accuracy: Train 95.5%, Val 99.0%         | MediaPipe keypoints simplify the input space, leading to a lightweight model with only 25k parameters. The GRU efficiently models the temporal dependencies in hand gestures, producing excellent generalization. This approach is computationally efficient, ideal for real-time applications, and robust due to keypoint-based input. |\n",
    "| 5         | MobileNetV2 + GRU (Pretrained) | Pretrained MobileNetV2, GRU (32 units), Dense (5 units), Dropout (50%)                  | Accuracy: Train 92.01%, Val 91.0%        | The pretrained MobileNetV2 effectively extracts spatial features, while the GRU layer models temporal dependencies. However, freezing the pretrained base limits the model’s ability to adapt to specific gesture tasks. Fine-tuning the base layers could further improve performance.                                                  |\n",
    "| 6         | MobileNetV3Small + GRU         | Pretrained MobileNetV3Small, GRU (64 units), Dense (5 units), Dropout (50%)             | Accuracy: Train 40%, Val 45%             | The frozen MobileNetV3Small base limits the model’s performance, resulting in underfitting. Temporal modeling via GRU is insufficient to compensate for the lack of fine-tuning. This model requires significant improvements through fine-tuning, data augmentation, or more robust temporal modeling.                                   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Best in the category\n",
    "\n",
    "| **Category**        | **Model**                     | **Key Hyperparameters**                                                                 | **Results**                             | **Why This Model Stands Out**                                                                                                                                                                                                                                                              |\n",
    "|----------------------|--------------------------------|-----------------------------------------------------------------------------------------|------------------------------------------|---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------|\n",
    "| **Non-Pretrained**   | Conv3D                        | 3 Conv3D layers (filters: 32, 64, 128), GlobalAvgPooling, Dense (128 units, dropout 50%) | Accuracy: Train 93.82%, Val 89.0%        | Conv3D achieves consistent performance with stable loss and accuracy trends. Its ability to learn spatial and temporal features simultaneously, without relying on pretrained weights, makes it a robust option. The stability suggests potential for improvement with additional training.           |\n",
    "| **Pretrained**       | GRU with MediaPipe Keypoints  | GRU (64 units), Flatten, Dense (5 units), Dropout (50%), MediaPipe Hand Keypoints       | Accuracy: Train 95.5%, Val 99.0%         | Leveraging MediaPipe’s pretrained keypoint extractor reduces input complexity, allowing the lightweight GRU-based model to achieve exceptional accuracy and efficiency. Its simplicity and computational efficiency make it ideal for real-time applications while maintaining robust generalization. |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and Save `utilities.py`\n",
    "\n",
    "The following script checks for the existence of `utilities.py`. If not found, it downloads the file from https://github.com/mohiteamit/upGrad-Gesture-Recognition\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download utilities.py\n",
    "import os\n",
    "import requests\n",
    "\n",
    "file_name = \"utilities.py\"\n",
    "url = \"https://raw.githubusercontent.com/mohiteamit/upGrad-Gesture-Recognition/refs/heads/main/utilities.py\"\n",
    "\n",
    "# Check if the file exists\n",
    "if not os.path.exists(file_name):\n",
    "    print(f\"{file_name} not found. Downloading...\")\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.ok:\n",
    "            with open(file_name, \"wb\") as file:\n",
    "                file.write(response.content)\n",
    "            print(f\"{file_name} downloaded successfully.\")\n",
    "        else:\n",
    "            print(f\"Failed to download {file_name}. HTTP Status Code: {response.status_code}\")\n",
    "            exit(1)\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {file_name}: {e}\")\n",
    "        exit(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download and Verify Models\n",
    "\n",
    "The script downloads models from a list of URLs into a specified directory, ensuring file integrity through verification. If a file is missing or corrupted, it is re-downloaded.\n",
    "\n",
    "- **Output Directory**: `models_to_evaluate`\n",
    "- **Model URLs**: Pre-defined list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Models downloaded.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "# List of model URLs\n",
    "model_urls = [\n",
    "    \"https://github.com/mohiteamit/upGrad-Gesture-Recognition/raw/refs/heads/main/best-models/Conv2D+GRU.keras\",\n",
    "    \"https://github.com/mohiteamit/upGrad-Gesture-Recognition/raw/refs/heads/main/best-models/Conv2D+LSTM.keras\",\n",
    "    \"https://github.com/mohiteamit/upGrad-Gesture-Recognition/raw/refs/heads/main/best-models/Conv3D-32-64-128.keras\",\n",
    "    \"https://github.com/mohiteamit/upGrad-Gesture-Recognition/raw/refs/heads/main/best-models/pretrained-MobileNetV2+GRU.keras\",\n",
    "    \"https://github.com/mohiteamit/upGrad-Gesture-Recognition/raw/refs/heads/main/best-models/pretrained-MobileNetV3Small+GRU.keras\",\n",
    "    \"https://github.com/mohiteamit/upGrad-Gesture-Recognition/raw/refs/heads/main/best-models/pretrained-mediapipe+gru.keras\",\n",
    "]\n",
    "\n",
    "# Directory to save models\n",
    "output_dir = \"models_to_evaluate\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Function to verify file integrity\n",
    "def verify_file(file_path, url):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        local_content = f.read()\n",
    "    response = requests.get(url)\n",
    "    return response.ok and local_content == response.content\n",
    "\n",
    "# Download models\n",
    "for url in model_urls:\n",
    "    filename = os.path.join(output_dir, os.path.basename(url))\n",
    "    try:\n",
    "        if not os.path.exists(filename) or not verify_file(filename, url):\n",
    "            response = requests.get(url)\n",
    "            if response.ok:\n",
    "                with open(filename, 'wb') as f:\n",
    "                    f.write(response.content)\n",
    "            else:\n",
    "                print(f\"Failed to download: {url}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {url}: {e}\")\n",
    "\n",
    "print(\"Models downloaded.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import Necessary Modules\n",
    "\n",
    "- **GestureDataGenerator**: Custom data generator from `utilities.py`.\n",
    "- **TensorFlow**: Framework for deep learning.\n",
    "- **load_model**: Used to load pre-trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities import GestureDataGenerator\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate `Conv2D+GRU` Model with TensorFlow 2.10.x\n",
    "\n",
    "The script checks for TensorFlow version compatibility and evaluates the `Conv2D+GRU` model using the `GestureDataGenerator`.\n",
    "\n",
    "- **TensorFlow Version**: `2.10.x`.\n",
    "- **Image Size**: `(120, 120)`\n",
    "- **Model Path**: `models_to_evaluate/Conv2D+GRU.keras`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 batches created, each of size 8, with 100 sequences of 30 images each. Use MediaPipe: False\n",
      "INFO:tensorflow:Mixed precision compatibility check (mixed_float16): OK\n",
      "Your GPUs will likely run quickly with dtype policy mixed_float16 as they all have compute capability of at least 7.0\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed_70 (TimeDi  (None, 30, None, None, 1  448      \n",
      " stributed)                  6)                                  \n",
      "                                                                 \n",
      " time_distributed_71 (TimeDi  (None, 30, None, None, 1  64       \n",
      " stributed)                  6)                                  \n",
      "                                                                 \n",
      " time_distributed_72 (TimeDi  (None, 30, None, None, 1  0        \n",
      " stributed)                  6)                                  \n",
      "                                                                 \n",
      " time_distributed_73 (TimeDi  (None, 30, None, None, 3  4640     \n",
      " stributed)                  2)                                  \n",
      "                                                                 \n",
      " time_distributed_74 (TimeDi  (None, 30, None, None, 3  128      \n",
      " stributed)                  2)                                  \n",
      "                                                                 \n",
      " time_distributed_75 (TimeDi  (None, 30, None, None, 3  0        \n",
      " stributed)                  2)                                  \n",
      "                                                                 \n",
      " time_distributed_76 (TimeDi  (None, 30, None, None, 6  18496    \n",
      " stributed)                  4)                                  \n",
      "                                                                 \n",
      " time_distributed_77 (TimeDi  (None, 30, None, None, 6  256      \n",
      " stributed)                  4)                                  \n",
      "                                                                 \n",
      " time_distributed_78 (TimeDi  (None, 30, None, None, 6  0        \n",
      " stributed)                  4)                                  \n",
      "                                                                 \n",
      " time_distributed_79 (TimeDi  (None, 30, 64)           0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " bidirectional_7 (Bidirectio  (None, 64)               18816     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 5)                 325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 47,333\n",
      "Trainable params: 47,109\n",
      "Non-trainable params: 224\n",
      "_________________________________________________________________\n",
      "13/13 [==============================] - 24s 2s/step - loss: 0.3106 - accuracy: 0.9400\n",
      "loss: 0.3106\n",
      "accuracy: 0.9400\n"
     ]
    }
   ],
   "source": [
    "if tf.__version__.startswith(\"2.10\"):\n",
    "    image_size = (120, 120)\n",
    "\n",
    "    test_generator = GestureDataGenerator(\n",
    "        data_path=test_data,\n",
    "        labels_csv=test_labels,\n",
    "        image_size=image_size\n",
    "    )\n",
    "\n",
    "    Conv2D_GRU = load_model('models_to_evaluate/Conv2D+GRU.keras')                   # Best image size 120x120\n",
    "    Conv2D_GRU.summary()\n",
    "    evaluation_results = Conv2D_GRU.evaluate(test_generator)\n",
    "    for metric, value in zip(Conv2D_GRU.metrics_names, evaluation_results):\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "else:\n",
    "    raise ValueError(\"This model requires TensorFlow 2.10.x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate `Conv2D+LSTM` Model\n",
    "\n",
    "The script checks for TensorFlow version compatibility and evaluates the `Conv2D+LSTM` model using the `GestureDataGenerator`.\n",
    "\n",
    "- **TensorFlow Version**: `2.10.x`.\n",
    "- **Image Size**: `(120, 120)`\n",
    "- **Model Path**: `models_to_evaluate/Conv2D+LSTM.keras`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 batches created, each of size 8, with 100 sequences of 30 images each. Use MediaPipe: False\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed_50 (TimeDi  (None, 30, None, None, 1  448      \n",
      " stributed)                  6)                                  \n",
      "                                                                 \n",
      " time_distributed_51 (TimeDi  (None, 30, None, None, 1  64       \n",
      " stributed)                  6)                                  \n",
      "                                                                 \n",
      " time_distributed_52 (TimeDi  (None, 30, None, None, 1  0        \n",
      " stributed)                  6)                                  \n",
      "                                                                 \n",
      " time_distributed_53 (TimeDi  (None, 30, None, None, 3  4640     \n",
      " stributed)                  2)                                  \n",
      "                                                                 \n",
      " time_distributed_54 (TimeDi  (None, 30, None, None, 3  128      \n",
      " stributed)                  2)                                  \n",
      "                                                                 \n",
      " time_distributed_55 (TimeDi  (None, 30, None, None, 3  0        \n",
      " stributed)                  2)                                  \n",
      "                                                                 \n",
      " time_distributed_56 (TimeDi  (None, 30, None, None, 6  18496    \n",
      " stributed)                  4)                                  \n",
      "                                                                 \n",
      " time_distributed_57 (TimeDi  (None, 30, None, None, 6  256      \n",
      " stributed)                  4)                                  \n",
      "                                                                 \n",
      " time_distributed_58 (TimeDi  (None, 30, None, None, 6  0        \n",
      " stributed)                  4)                                  \n",
      "                                                                 \n",
      " time_distributed_59 (TimeDi  (None, 30, 64)           0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " bidirectional_5 (Bidirectio  (None, 64)               18816     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 5)                 325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 47,333\n",
      "Trainable params: 47,109\n",
      "Non-trainable params: 224\n",
      "_________________________________________________________________\n",
      "13/13 [==============================] - 6s 380ms/step - loss: 0.3660 - accuracy: 0.9100\n",
      "loss: 0.3660\n",
      "accuracy: 0.9100\n"
     ]
    }
   ],
   "source": [
    "if tf.__version__.startswith(\"2.10\"):\n",
    "    image_size = (120, 120)\n",
    "\n",
    "    test_generator = GestureDataGenerator(\n",
    "        data_path=test_data,\n",
    "        labels_csv=test_labels,\n",
    "        image_size=image_size\n",
    "    )\n",
    "\n",
    "    Conv2D_LSTM = load_model('models_to_evaluate/Conv2D+LSTM.keras')\n",
    "    Conv2D_LSTM.summary()\n",
    "    evaluation_results = Conv2D_LSTM.evaluate(test_generator)\n",
    "    for metric, value in zip(Conv2D_LSTM.metrics_names, evaluation_results):\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "else:\n",
    "    raise ValueError(\"This model requires TensorFlow 2.10.x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate `Conv3D` Model - THE BEST MODEL WITHOUT PRE-TRAIN\n",
    "\n",
    "`The best performing model without transfer learning`\n",
    "\n",
    "The script checks for TensorFlow version compatibility and evaluates the `Conv3D` model using the `GestureDataGenerator`.\n",
    "\n",
    "- **TensorFlow Version**: `2.10.x`.\n",
    "- **Image Size**: `(200, 200)`\n",
    "- **Model Path**: `models_to_evaluate/Conv3D-32-64-128.keras`\n",
    "\n",
    "**Note:** model has shown potential to perform even better with more training data and additional epochs. Conv2D+GRU and Conv2D+LSTM show better scores (94% and 91% respectively) but are less predictable in thier loss. Conv3D however is stable and will perform equally well on unseen data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 batches created, each of size 8, with 100 sequences of 30 images each. Use MediaPipe: False\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d_3 (Conv3D)           (None, 30, 200, 200, 32)  2624      \n",
      "                                                                 \n",
      " max_pooling3d_2 (MaxPooling  (None, 15, 100, 100, 32)  0        \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_4 (Conv3D)           (None, 15, 100, 100, 64)  55360     \n",
      "                                                                 \n",
      " max_pooling3d_3 (MaxPooling  (None, 8, 50, 50, 64)    0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_5 (Conv3D)           (None, 8, 50, 50, 128)    221312    \n",
      "                                                                 \n",
      " global_average_pooling3d_1   (None, 128)              0         \n",
      " (GlobalAveragePooling3D)                                        \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 296,453\n",
      "Trainable params: 296,453\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "13/13 [==============================] - 6s 458ms/step - loss: 0.3709 - accuracy: 0.8900\n",
      "loss: 0.3709\n",
      "accuracy: 0.8900\n"
     ]
    }
   ],
   "source": [
    "if tf.__version__.startswith(\"2.10\"):\n",
    "    image_size = (200, 200)\n",
    "\n",
    "    test_generator = GestureDataGenerator(\n",
    "        data_path=test_data,\n",
    "        labels_csv=test_labels,\n",
    "        image_size=image_size\n",
    "    )\n",
    "    Conv3D_32_64_128 = load_model('models_to_evaluate/Conv3D-32-64-128.keras') \n",
    "    Conv3D_32_64_128.summary()\n",
    "    evaluation_results = Conv3D_32_64_128.evaluate(test_generator)\n",
    "    for metric, value in zip(Conv3D_32_64_128.metrics_names, evaluation_results):\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "else:\n",
    "    raise ValueError(\"This model requires TensorFlow 2.10.x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate `MobileNetV2+GRU` Model\n",
    "\n",
    "The following code evaluates the `MobileNetV2+GRU` model using TensorFlow 2.18.x on test data.\n",
    "\n",
    "- **TensorFlow Version**: `2.18.x`.\n",
    "- **Image Size**: `(224, 224)`\n",
    "- **Use MediaPipe**: `False`\n",
    "- **Model Path**: `models_to_evaluate/pretrained-MobileNetV2+GRU.keras`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 batches created, each of size 8, with 100 sequences of 30 images each. Use MediaPipe: False\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"CNN_GRU_Model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"CNN_GRU_Model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ CNN_Layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>) │     <span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ MaxPooling_Layer                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>) │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Flatten_Layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">11520</span>)      │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ GRU_Layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,109,184</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Dropout_Layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Output_Layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">165</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ CNN_Layer (\u001b[38;5;33mTimeDistributed\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m) │     \u001b[38;5;34m2,257,984\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ MaxPooling_Layer                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m1280\u001b[0m) │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Flatten_Layer (\u001b[38;5;33mTimeDistributed\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m11520\u001b[0m)      │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ GRU_Layer (\u001b[38;5;33mGRU\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │     \u001b[38;5;34m1,109,184\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Dropout_Layer (\u001b[38;5;33mDropout\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Output_Layer (\u001b[38;5;33mDense\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)              │           \u001b[38;5;34m165\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,586,033</span> (21.31 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,586,033\u001b[0m (21.31 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,109,349</span> (4.23 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,109,349\u001b[0m (4.23 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,257,984</span> (8.61 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,257,984\u001b[0m (8.61 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,218,700</span> (8.46 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2,218,700\u001b[0m (8.46 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m53s\u001b[0m 2s/step - accuracy: 0.8174 - loss: 0.8290\n",
      "loss: 0.8503\n",
      "compile_metrics: 0.8200\n"
     ]
    }
   ],
   "source": [
    "if tf.__version__.startswith(\"2.18\"):\n",
    "    image_size = (224, 224)\n",
    "\n",
    "    test_generator = GestureDataGenerator(\n",
    "        data_path=test_data,\n",
    "        labels_csv=test_labels,\n",
    "        image_size=image_size,\n",
    "        use_mediapipe=False\n",
    "    )\n",
    "\n",
    "    MobileNetV2_GRU = load_model('models_to_evaluate/pretrained-MobileNetV2+GRU.keras')\n",
    "    MobileNetV2_GRU.summary()\n",
    "    evaluation_results = MobileNetV2_GRU.evaluate(test_generator)\n",
    "    for metric, value in zip(MobileNetV2_GRU.metrics_names, evaluation_results):\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "else:\n",
    "    raise ValueError(\"This model requires TensorFlow 2.18.x\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate `MobileNetV3Small+GRU` Model\n",
    "\n",
    "The following evaluates the `MobileNetV3Small+GRU` model using TensorFlow 2.18.x on test data.\n",
    "\n",
    "- **TensorFlow Version**: `2.18`.\n",
    "- **Image Size**: `(224, 224)`\n",
    "- **Model Path**: `models_to_evaluate/pretrained-MobileNetV3Small+GRU.keras`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 batches created, each of size 8, with 100 sequences of 30 images each. Use MediaPipe: False\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"CNN_GRU_Model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"CNN_GRU_Model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ CNN_Layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">576</span>)  │       <span style=\"color: #00af00; text-decoration-color: #00af00\">939,120</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ MaxPooling_Layer                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">576</span>)  │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Flatten_Layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5184</span>)       │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ GRU_Layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,008,000</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Dropout_Layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Output_Layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">325</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ CNN_Layer (\u001b[38;5;33mTimeDistributed\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m576\u001b[0m)  │       \u001b[38;5;34m939,120\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ MaxPooling_Layer                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m3\u001b[0m, \u001b[38;5;34m576\u001b[0m)  │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Flatten_Layer (\u001b[38;5;33mTimeDistributed\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m5184\u001b[0m)       │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ GRU_Layer (\u001b[38;5;33mGRU\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │     \u001b[38;5;34m1,008,000\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Dropout_Layer (\u001b[38;5;33mDropout\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Output_Layer (\u001b[38;5;33mDense\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)              │           \u001b[38;5;34m325\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">3,964,097</span> (15.12 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m3,964,097\u001b[0m (15.12 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,008,325</span> (3.85 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,008,325\u001b[0m (3.85 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">939,120</span> (3.58 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m939,120\u001b[0m (3.58 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,016,652</span> (7.69 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2,016,652\u001b[0m (7.69 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 847ms/step - accuracy: 0.3608 - loss: 1.4221\n",
      "loss: 1.4710\n",
      "compile_metrics: 0.3600\n"
     ]
    }
   ],
   "source": [
    "if tf.__version__.startswith(\"2.18\"):\n",
    "    image_size = (224, 224)\n",
    "\n",
    "    test_generator = GestureDataGenerator(\n",
    "        data_path=test_data,\n",
    "        labels_csv=test_labels,\n",
    "        image_size=image_size,\n",
    "    )\n",
    "\n",
    "    MobileNetV3Small_GRU = load_model('models_to_evaluate/pretrained-MobileNetV3Small+GRU.keras')\n",
    "    MobileNetV3Small_GRU.summary()\n",
    "    evaluation_results = MobileNetV3Small_GRU.evaluate(test_generator)\n",
    "    for metric, value in zip(MobileNetV3Small_GRU.metrics_names, evaluation_results):\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "else:\n",
    "    raise ValueError(\"This model requires TensorFlow 2.18.x\")        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate `Mediapipe+GRU` Model - THE BEST MODEL WITH PRE-TRAIN\n",
    "\n",
    "`The best performing model with transfer learning`\n",
    "\n",
    "The following code evaluates the `Mediapipe+GRU` model using TensorFlow 2.18.x on test data. Model usages mediapipe hand as part of data processing step to compact images into (21, 3) array representing 21 key points of the hand.\n",
    "\n",
    "- **TensorFlow Version**: Must start with `2.18`.\n",
    "- **Image Size**: `(256, 256)`\n",
    "- **Use MediaPipe**: `True`\n",
    "- **Model Path**: `models_to_evaluate/pretrained-mediapipe+gru.keras`\n",
    "\n",
    "**Note:** depending on how this model is deployed in practice performance of the model will differ. However this model will always out-done any other model by only focusing on hands and ignoring all other noise. Model is also CPU centric and does not require GPU centric hardware for predicating single hand gesture at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 batches created, each of size 8, with 100 sequences of 30 images each. Use MediaPipe: True\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"GRU_Gesture_Model\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"GRU_Gesture_Model\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)                    </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">       Param # </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ Flatten_Keypoints               │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">30</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">63</span>)         │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">TimeDistributed</span>)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ GRU_Layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GRU</span>)                 │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │        <span style=\"color: #00af00; text-decoration-color: #00af00\">24,768</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Dropout_Layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)             │             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Output_Layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">5</span>)              │           <span style=\"color: #00af00; text-decoration-color: #00af00\">325</span> │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━┩\n",
       "│ Flatten_Keypoints               │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m30\u001b[0m, \u001b[38;5;34m63\u001b[0m)         │             \u001b[38;5;34m0\u001b[0m │\n",
       "│ (\u001b[38;5;33mTimeDistributed\u001b[0m)               │                        │               │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ GRU_Layer (\u001b[38;5;33mGRU\u001b[0m)                 │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │        \u001b[38;5;34m24,768\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Dropout_Layer (\u001b[38;5;33mDropout\u001b[0m)         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)             │             \u001b[38;5;34m0\u001b[0m │\n",
       "├─────────────────────────────────┼────────────────────────┼───────────────┤\n",
       "│ Output_Layer (\u001b[38;5;33mDense\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m5\u001b[0m)              │           \u001b[38;5;34m325\u001b[0m │\n",
       "└─────────────────────────────────┴────────────────────────┴───────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,093</span> (98.02 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m25,093\u001b[0m (98.02 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">25,093</span> (98.02 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m25,093\u001b[0m (98.02 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m13/13\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m57s\u001b[0m 4s/step - accuracy: 0.8749 - loss: 0.4942\n",
      "loss: 0.3874\n",
      "compile_metrics: 0.9300\n"
     ]
    }
   ],
   "source": [
    "if tf.__version__.startswith(\"2.18\"):\n",
    "    image_size = (256, 256)\n",
    "\n",
    "    test_generator = GestureDataGenerator(\n",
    "        data_path=test_data,\n",
    "        labels_csv=test_labels,\n",
    "        image_size=image_size,\n",
    "        use_mediapipe=True\n",
    "    )\n",
    "\n",
    "    mediapipe_GRU = load_model('models_to_evaluate/pretrained-mediapipe+gru.keras')\n",
    "    mediapipe_GRU.compile(optimizer=\"adam\", loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
    "    mediapipe_GRU.summary()\n",
    "    evaluation_results = mediapipe_GRU.evaluate(test_generator)\n",
    "    for metric, value in zip(mediapipe_GRU.metrics_names, evaluation_results):\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "else:\n",
    "    raise ValueError(\"This model requires TensorFlow 2.18.x\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

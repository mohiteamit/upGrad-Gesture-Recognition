{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = './data/val'\n",
    "test_labels = './data/val.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "# File name and URL\n",
    "file_name = \"utilities.py\"\n",
    "url = \"https://raw.githubusercontent.com/mohiteamit/upGrad-Gesture-Recognition/refs/heads/main/utilities.py\"\n",
    "\n",
    "# Check if the file exists\n",
    "if not os.path.exists(file_name):\n",
    "    print(f\"{file_name} not found. Downloading...\")\n",
    "    try:\n",
    "        response = requests.get(url)\n",
    "        if response.ok:\n",
    "            with open(file_name, \"wb\") as file:\n",
    "                file.write(response.content)\n",
    "            print(f\"{file_name} downloaded successfully.\")\n",
    "        else:\n",
    "            print(f\"Failed to download {file_name}. HTTP Status Code: {response.status_code}\")\n",
    "            exit(1)\n",
    "    except Exception as e:\n",
    "        print(f\"Error downloading {file_name}: {e}\")\n",
    "        exit(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Failed to download: https://github.com/mohiteamit/upGrad-Gesture-Recognition/raw/refs/heads/main/best-models/pretrained-MobileNetV2+GRU.h5\n",
      "Failed to download: https://github.com/mohiteamit/upGrad-Gesture-Recognition/raw/refs/heads/main/best-models/pretrained-MobileNetV3Small+GRU.h5\n",
      "Failed to download: https://github.com/mohiteamit/upGrad-Gesture-Recognition/raw/refs/heads/main/best-models/pretrained-mediapipe+gru.h5\n",
      "Models downloaded.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import requests\n",
    "\n",
    "# List of model URLs\n",
    "model_urls = [\n",
    "    \"https://github.com/mohiteamit/upGrad-Gesture-Recognition/raw/refs/heads/main/best-models/Conv2D+GRU.h5\",\n",
    "    \"https://github.com/mohiteamit/upGrad-Gesture-Recognition/raw/refs/heads/main/best-models/Conv2D+LSTM.h5\",\n",
    "    \"https://github.com/mohiteamit/upGrad-Gesture-Recognition/raw/refs/heads/main/best-models/Conv3D-32-64-128.h5\",\n",
    "    \"https://github.com/mohiteamit/upGrad-Gesture-Recognition/raw/refs/heads/main/best-models/pretrained-MobileNetV2+GRU.h5\",\n",
    "    \"https://github.com/mohiteamit/upGrad-Gesture-Recognition/raw/refs/heads/main/best-models/pretrained-MobileNetV3Small+GRU.h5\",\n",
    "    \"https://github.com/mohiteamit/upGrad-Gesture-Recognition/raw/refs/heads/main/best-models/pretrained-mediapipe+gru.h5\",\n",
    "]\n",
    "\n",
    "# Directory to save models\n",
    "output_dir = \"models_to_evaluate\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Function to verify file integrity\n",
    "def verify_file(file_path, url):\n",
    "    with open(file_path, 'rb') as f:\n",
    "        local_content = f.read()\n",
    "    response = requests.get(url)\n",
    "    return response.ok and local_content == response.content\n",
    "\n",
    "# Download models\n",
    "for url in model_urls:\n",
    "    filename = os.path.join(output_dir, os.path.basename(url))\n",
    "    try:\n",
    "        if not os.path.exists(filename) or not verify_file(filename, url):\n",
    "            response = requests.get(url)\n",
    "            if response.ok:\n",
    "                with open(filename, 'wb') as f:\n",
    "                    f.write(response.content)\n",
    "            else:\n",
    "                print(f\"Failed to download: {url}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {url}: {e}\")\n",
    "\n",
    "print(\"Models downloaded.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "def convert_keras_to_h5(directory):\n",
    "    # Iterate through all files in the directory\n",
    "    for file in os.listdir(directory):\n",
    "        if file.endswith(\".keras\"):\n",
    "            keras_file = os.path.join(directory, file)\n",
    "            try:\n",
    "                # Try loading the .keras file\n",
    "                model = load_model(keras_file)\n",
    "                # Save the model as .h5\n",
    "                h5_file = os.path.join(directory, file.replace(\".keras\", \".h5\"))\n",
    "                model.save(h5_file)\n",
    "                print(f\"Converted: {keras_file} to {h5_file}\")\n",
    "            except Exception as e:\n",
    "                # If loading fails, print an error message\n",
    "                print(f\"Failed to load {keras_file}: {e}\")\n",
    "\n",
    "# Run the conversion process\n",
    "convert_keras_to_h5(\"models_to_evaluate\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utilities import GestureDataGenerator\n",
    "from tensorflow.keras.models import load_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 batches created, each of size 8, with 100 sequences of 30 images each. Use MediaPipe: False\n",
      "Model: \"sequential_7\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed_70 (TimeDi  (None, 30, None, None, 1  448      \n",
      " stributed)                  6)                                  \n",
      "                                                                 \n",
      " time_distributed_71 (TimeDi  (None, 30, None, None, 1  64       \n",
      " stributed)                  6)                                  \n",
      "                                                                 \n",
      " time_distributed_72 (TimeDi  (None, 30, None, None, 1  0        \n",
      " stributed)                  6)                                  \n",
      "                                                                 \n",
      " time_distributed_73 (TimeDi  (None, 30, None, None, 3  4640     \n",
      " stributed)                  2)                                  \n",
      "                                                                 \n",
      " time_distributed_74 (TimeDi  (None, 30, None, None, 3  128      \n",
      " stributed)                  2)                                  \n",
      "                                                                 \n",
      " time_distributed_75 (TimeDi  (None, 30, None, None, 3  0        \n",
      " stributed)                  2)                                  \n",
      "                                                                 \n",
      " time_distributed_76 (TimeDi  (None, 30, None, None, 6  18496    \n",
      " stributed)                  4)                                  \n",
      "                                                                 \n",
      " time_distributed_77 (TimeDi  (None, 30, None, None, 6  256      \n",
      " stributed)                  4)                                  \n",
      "                                                                 \n",
      " time_distributed_78 (TimeDi  (None, 30, None, None, 6  0        \n",
      " stributed)                  4)                                  \n",
      "                                                                 \n",
      " time_distributed_79 (TimeDi  (None, 30, 64)           0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " bidirectional_7 (Bidirectio  (None, 64)               18816     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_14 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_7 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_15 (Dense)            (None, 5)                 325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 47,333\n",
      "Trainable params: 47,109\n",
      "Non-trainable params: 224\n",
      "_________________________________________________________________\n",
      "13/13 [==============================] - 5s 360ms/step - loss: 0.3106 - accuracy: 0.9400\n",
      "loss: 0.3106\n",
      "accuracy: 0.9400\n"
     ]
    }
   ],
   "source": [
    "load_fraction = 1.0\n",
    "image_size = (120, 120)\n",
    "\n",
    "test_generator = GestureDataGenerator(\n",
    "    data_path=test_data,\n",
    "    labels_csv=test_labels,\n",
    "    image_size=image_size,\n",
    "    debug=False,\n",
    ")\n",
    "\n",
    "Conv2D_GRU = load_model('models_to_evaluate/Conv2D+GRU.h5')                   # Best image size 120x120\n",
    "Conv2D_GRU.summary()\n",
    "evaluation_results = Conv2D_GRU.evaluate(test_generator)\n",
    "for metric, value in zip(Conv2D_GRU.metrics_names, evaluation_results):\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 batches created, each of size 64, with 100 sequences of 30 images each. Use MediaPipe: False\n",
      "Model: \"sequential_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed_50 (TimeDi  (None, 30, None, None, 1  448      \n",
      " stributed)                  6)                                  \n",
      "                                                                 \n",
      " time_distributed_51 (TimeDi  (None, 30, None, None, 1  64       \n",
      " stributed)                  6)                                  \n",
      "                                                                 \n",
      " time_distributed_52 (TimeDi  (None, 30, None, None, 1  0        \n",
      " stributed)                  6)                                  \n",
      "                                                                 \n",
      " time_distributed_53 (TimeDi  (None, 30, None, None, 3  4640     \n",
      " stributed)                  2)                                  \n",
      "                                                                 \n",
      " time_distributed_54 (TimeDi  (None, 30, None, None, 3  128      \n",
      " stributed)                  2)                                  \n",
      "                                                                 \n",
      " time_distributed_55 (TimeDi  (None, 30, None, None, 3  0        \n",
      " stributed)                  2)                                  \n",
      "                                                                 \n",
      " time_distributed_56 (TimeDi  (None, 30, None, None, 6  18496    \n",
      " stributed)                  4)                                  \n",
      "                                                                 \n",
      " time_distributed_57 (TimeDi  (None, 30, None, None, 6  256      \n",
      " stributed)                  4)                                  \n",
      "                                                                 \n",
      " time_distributed_58 (TimeDi  (None, 30, None, None, 6  0        \n",
      " stributed)                  4)                                  \n",
      "                                                                 \n",
      " time_distributed_59 (TimeDi  (None, 30, 64)           0         \n",
      " stributed)                                                      \n",
      "                                                                 \n",
      " bidirectional_5 (Bidirectio  (None, 64)               18816     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 5)                 325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 47,333\n",
      "Trainable params: 47,109\n",
      "Non-trainable params: 224\n",
      "_________________________________________________________________\n",
      "2/2 [==============================] - 6s 3s/step - loss: 0.3660 - accuracy: 0.9100\n",
      "loss: 0.3660\n",
      "accuracy: 0.9100\n"
     ]
    }
   ],
   "source": [
    "load_fraction = 1.0\n",
    "batch_size = 64\n",
    "image_size = (120, 120)\n",
    "\n",
    "test_generator = GestureDataGenerator(\n",
    "    data_path=test_data,\n",
    "    labels_csv=test_labels,\n",
    "    batch_size=batch_size,\n",
    "    image_size=image_size,\n",
    "    augmentations=None,\n",
    "    shuffle=False,\n",
    "    load_fraction=1.0,\n",
    "    debug=False,\n",
    "    use_mediapipe=False,\n",
    ")\n",
    "\n",
    "Conv2D_LSTM = load_model('models_to_evaluate/Conv2D+LSTM.h5')                   # Best image size 120x120\n",
    "Conv2D_LSTM.summary()\n",
    "evaluation_results = Conv2D_LSTM.evaluate(test_generator)\n",
    "for metric, value in zip(Conv2D_LSTM.metrics_names, evaluation_results):\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13 batches created, each of size 8, with 100 sequences of 30 images each. Use MediaPipe: False\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv3d_3 (Conv3D)           (None, 30, 200, 200, 32)  2624      \n",
      "                                                                 \n",
      " max_pooling3d_2 (MaxPooling  (None, 15, 100, 100, 32)  0        \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_4 (Conv3D)           (None, 15, 100, 100, 64)  55360     \n",
      "                                                                 \n",
      " max_pooling3d_3 (MaxPooling  (None, 8, 50, 50, 64)    0         \n",
      " 3D)                                                             \n",
      "                                                                 \n",
      " conv3d_5 (Conv3D)           (None, 8, 50, 50, 128)    221312    \n",
      "                                                                 \n",
      " global_average_pooling3d_1   (None, 128)              0         \n",
      " (GlobalAveragePooling3D)                                        \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 128)               16512     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 5)                 645       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 296,453\n",
      "Trainable params: 296,453\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "13/13 [==============================] - 6s 456ms/step - loss: 0.3709 - accuracy: 0.8900\n",
      "loss: 0.3709\n",
      "accuracy: 0.8900\n"
     ]
    }
   ],
   "source": [
    "batch_size = 1\n",
    "image_size = (200, 200)\n",
    "\n",
    "test_generator = GestureDataGenerator(\n",
    "    data_path=test_data,\n",
    "    labels_csv=test_labels,\n",
    "    image_size=image_size,\n",
    "    debug=False,\n",
    ")\n",
    "\n",
    "Conv3D_32_64_128 = load_model('models_to_evaluate/Conv3D-32-64-128.h5')                   # Best image size 120x120\n",
    "Conv3D_32_64_128.summary()\n",
    "evaluation_results = Conv3D_32_64_128.evaluate(test_generator)\n",
    "for metric, value in zip(Conv3D_32_64_128.metrics_names, evaluation_results):\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mediapipe_GRU = load_model('models_to_evaluate/pretrained-mediapipe+gru.keras')                   # Best image size 120x120\n",
    "# mediapipe_GRU.summary()\n",
    "# evaluation_results = mediapipe_GRU.evaluate(test_generator)\n",
    "# for metric, value in zip(mediapipe_GRU.metrics_names, evaluation_results):\n",
    "#     print(f\"{metric}: {value:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MobileNetV2_GRU = load_model('best-models/pretrained-MobileNetV2+GRU.keras')                   # Best image size 120x120\n",
    "# MobileNetV2_GRU.summary()\n",
    "# evaluation_results = MobileNetV2_GRU.evaluate(test_generator)\n",
    "# for metric, value in zip(MobileNetV2_GRU.metrics_names, evaluation_results):\n",
    "#     print(f\"{metric}: {value:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

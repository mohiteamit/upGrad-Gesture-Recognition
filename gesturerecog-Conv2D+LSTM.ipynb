{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = './data/train'\n",
    "train_labels = './data/train.csv'\n",
    "val_data = './data/val'\n",
    "val_labels = './data/val.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 23:27:10.237973: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-02 23:27:10.339726: I tensorflow/core/util/util.cc:169] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Random seed set to 42\n"
     ]
    }
   ],
   "source": [
    "from utilities import GestureDataGenerator, plot_training_history, set_seed, get_callbacks, set_memorry_limit_for_tf\n",
    "set_seed(seed=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42 batches created, each of size 16, with 663 sequences of 30 images each. Use MediaPipe: False\n",
      "7 batches created, each of size 16, with 100 sequences of 30 images each. Use MediaPipe: False\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input batch shape (X): (16, 30, 120, 120, 3)\n",
      "Labels batch shape (y): (16, 5)\n",
      "First label in batch (one-hot): [0. 0. 0. 1. 0.]\n"
     ]
    }
   ],
   "source": [
    "load_fraction = 1.0             # Full data load\n",
    "batch_size = 16                 # We are low on resources. We will go slow and steady.\n",
    "image_size = (120, 120)\n",
    "\n",
    "# Initialize the generator\n",
    "train_generator = GestureDataGenerator(\n",
    "    data_path=train_data,\n",
    "    labels_csv=train_labels,\n",
    "    batch_size=batch_size,\n",
    "    image_size=image_size,\n",
    "    augmentations={\n",
    "        'rotation': 5,       # Rotate up to Â±5 degrees\n",
    "        'brightness': True,  # Random brightness adjustment\n",
    "        'contrast': True,    # Random contrast adjustment\n",
    "        'blur': True         # Apply Gaussian blur\n",
    "    },    \n",
    "    shuffle=True,\n",
    "    load_fraction=load_fraction,\n",
    "    debug=False,\n",
    "    use_mediapipe=False,\n",
    ")\n",
    "\n",
    "val_generator = GestureDataGenerator(\n",
    "    data_path=val_data,\n",
    "    labels_csv=val_labels,\n",
    "    batch_size=batch_size,\n",
    "    image_size=image_size,\n",
    "    augmentations=None,\n",
    "    shuffle=False,\n",
    "    load_fraction=1.0,\n",
    "    debug=False,\n",
    "    use_mediapipe=False,\n",
    ")\n",
    "\n",
    "# Get the first batch\n",
    "X, y = train_generator[0]\n",
    "\n",
    "# Print outputs\n",
    "print(\"Input batch shape (X):\", X.shape)  # Expected shape: (batch_size, sequence_length, 224, 224, 3)\n",
    "print(\"Labels batch shape (y):\", y.shape)  # Expected shape: (batch_size, num_classes)\n",
    "print(\"First label in batch (one-hot):\", y[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters\n",
    "sequence_length = train_generator.sequence_length  # Frames per video (from generator)\n",
    "image_size = train_generator.image_size            # Image size (height, width)\n",
    "num_classes = train_generator.num_classes          # Number of gesture classes\n",
    "input_shape = (sequence_length, image_size[0], image_size[1], 3)  # Conv3D input shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n",
      "WARNING:tensorflow:Layer lstm will not use cuDNN kernels since it doesn't meet the criteria. It will use a generic GPU kernel as fallback when running on GPU.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-12-02 23:27:13.445924: I tensorflow/core/platform/cpu_feature_guard.cc:194] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  SSE3 SSE4.1 SSE4.2 AVX\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-12-02 23:27:13.896423: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1637] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 1123 MB memory:  -> device: 0, name: NVIDIA RTX 6000 Ada Generation, pci bus id: 0000:e1:00.0, compute capability: 8.9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " time_distributed (TimeDistr  (None, 30, 120, 120, 16)  448      \n",
      " ibuted)                                                         \n",
      "                                                                 \n",
      " time_distributed_1 (TimeDis  (None, 30, 120, 120, 16)  64       \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_2 (TimeDis  (None, 30, 60, 60, 16)   0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_3 (TimeDis  (None, 30, 60, 60, 32)   4640      \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_4 (TimeDis  (None, 30, 60, 60, 32)   128       \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_5 (TimeDis  (None, 30, 30, 30, 32)   0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_6 (TimeDis  (None, 30, 30, 30, 64)   18496     \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_7 (TimeDis  (None, 30, 30, 30, 64)   256       \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_8 (TimeDis  (None, 30, 15, 15, 64)   0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " time_distributed_9 (TimeDis  (None, 30, 64)           0         \n",
      " tributed)                                                       \n",
      "                                                                 \n",
      " bidirectional (Bidirectiona  (None, 64)               24832     \n",
      " l)                                                              \n",
      "                                                                 \n",
      " dense (Dense)               (None, 64)                4160      \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 64)                0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 5)                 325       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 53,349\n",
      "Trainable params: 53,125\n",
      "Non-trainable params: 224\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras import layers, models, regularizers\n",
    "\n",
    "# Define the model\n",
    "model = models.Sequential([\n",
    "    # Input layer\n",
    "    layers.Input(shape=input_shape),  # Input shape: (timesteps, height, width, channels)\n",
    "\n",
    "    # Smaller CNN layers for feature extraction\n",
    "    layers.TimeDistributed(layers.Conv2D(16, (3, 3), activation='relu', padding='same')),\n",
    "    layers.TimeDistributed(layers.BatchNormalization()),\n",
    "    layers.TimeDistributed(layers.MaxPooling2D(pool_size=(2, 2), padding='same')),\n",
    "\n",
    "    layers.TimeDistributed(layers.Conv2D(32, (3, 3), activation='relu', padding='same')),\n",
    "    layers.TimeDistributed(layers.BatchNormalization()),\n",
    "    layers.TimeDistributed(layers.MaxPooling2D(pool_size=(2, 2), padding='same')),\n",
    "\n",
    "    layers.TimeDistributed(layers.Conv2D(64, (3, 3), activation='relu', padding='same')),\n",
    "    layers.TimeDistributed(layers.BatchNormalization()),\n",
    "    layers.TimeDistributed(layers.MaxPooling2D(pool_size=(2, 2), padding='same')),\n",
    "\n",
    "    # Global average pooling to reduce parameters\n",
    "    layers.TimeDistributed(layers.GlobalAveragePooling2D()),\n",
    "\n",
    "    # RNN layer for temporal modeling\n",
    "    layers.Bidirectional(layers.LSTM(32, return_sequences=False, dropout=0.3, recurrent_dropout=0.3)),\n",
    "\n",
    "    # Fully connected layers\n",
    "    layers.Dense(64, activation='relu', kernel_regularizer=regularizers.l2(0.01)),  # L2 regularization\n",
    "    layers.Dropout(0.4),\n",
    "\n",
    "    # Output layer\n",
    "    layers.Dense(num_classes, activation='softmax')  # Softmax for multi-class classification\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer='adam', \n",
    "    loss='categorical_crossentropy', \n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Print the model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_location = './best-models/Conv2D+LSTM.keras'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint_callback, reduce_lr_callback, early_stopping_callback = get_callbacks(filepath = model_save_location)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train the model\n",
    "history_1 = model.fit(\n",
    "    x=train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=5,\n",
    "    callbacks=[checkpoint_callback, reduce_lr_callback, early_stopping_callback],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "plot_training_history(histories=[history_1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(model_save_location)\n",
    "\n",
    "# Train the model for more epochs\n",
    "history_2 = model.fit(\n",
    "    x=train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=30,                  # Train for more epochs\n",
    "    initial_epoch=5,            # Start counting previous epochs\n",
    "    callbacks=[checkpoint_callback, reduce_lr_callback, early_stopping_callback],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "plot_training_history(histories=[history_1, history_2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(model_save_location)\n",
    "\n",
    "# Train the model for more epochs\n",
    "history_3 = model.fit(\n",
    "    x=train_generator,\n",
    "    validation_data=val_generator,\n",
    "    epochs=50,                   # Train for more epochs\n",
    "    initial_epoch=30,            # Start counting previous epochs\n",
    "    callbacks=[checkpoint_callback, reduce_lr_callback, early_stopping_callback],\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "plot_training_history(histories=[history_1, history_2, history_3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_weights(model_save_location)\n",
    "\n",
    "evaluation_results = model.evaluate(val_generator, verbose=1)\n",
    "\n",
    "for metric, value in zip(model.metrics_names, evaluation_results):\n",
    "    print(f\"{metric}: {value:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tensorflow_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
